{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc845879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import VisionTransformer, CONFIGS, AutoEncoder, ResNet50, InputEmbed\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "# from dataset import get_dataloader\n",
    "from dataset import make_word_img, get_img, get_color\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from visualization import visualize,get_word_img,get_visualization_res\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b006055a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d32172",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'Transformer'\n",
    "NUM_EPOCHS = 1000\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = 'cuda'\n",
    "EARLY_STOP = True\n",
    "NUM_LAYER = 4\n",
    "EARLY_STOP_NUM = 10\n",
    "MODEL_SIZE = f\"head6_layer{NUM_LAYER}\"\n",
    "MODEL_NAME = '1' #ResNet50\n",
    "LR = 1e-5\n",
    "SAVE_ROOT = \"results_for_wo_one_element\"\n",
    "SAVE_FOLDAR = f'{MODEL}_{MODEL_SIZE}_lr{LR}'\n",
    "DROP_ELEMENTS = [\"font_style\",\"char_color\",\"bk_color\",\"height\",\"coord\"]\n",
    "ELEMENTS = [\"semantic\",\"font_style\",\"char_color\",\"bk_color\",\"height\",\"coord\"]\n",
    "ttf_path = \"/workspace/dataset/OpenSans-Regular.ttf\"\n",
    "\n",
    "\n",
    "for dp_elemet in DROP_ELEMENTS:\n",
    "    ELEMENTS.remove(dp_elemet)\n",
    "    SAVE_FOLDAR += f'_{dp_elemet}'\n",
    "CONFIGS[\"title_level_config\"][\"transformer\"][\"num_layers\"] = NUM_LAYER\n",
    "def torch_fix_seed(seed=0):\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorchsemantic\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms = True\n",
    "torch_fix_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4973e491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_level_config = CONFIGS[\"title_level_config\"]\n",
    "word_level_config = CONFIGS[\"word_level_config\"]\n",
    "model_title_level = VisionTransformer(title_level_config, num_classes=32, zero_head=False, vis=True).to(DEVICE)\n",
    "model_word_level = VisionTransformer(word_level_config, num_classes=32, zero_head=False, vis=True).to(DEVICE)\n",
    "model_word_level.head = nn.Identity()\n",
    "model_word_level.fc = nn.Identity()\n",
    "\n",
    "semantic_model_path = '/workspace/dataset/GoogleNews-vectors-negative300.bin.gz'\n",
    "model_semantic = gensim.models.KeyedVectors.load_word2vec_format(semantic_model_path, binary=True)\n",
    "\n",
    "\n",
    "embed_xy = InputEmbed(input_size=2,output_size=300).to(DEVICE)\n",
    "embed_size = InputEmbed(input_size=1,output_size=300).to(DEVICE)\n",
    "\n",
    "model_path = f'/workspace/{SAVE_ROOT}/{SAVE_FOLDAR}/best_model_word_level.pth'\n",
    "model_word_level.load_state_dict(torch.load(model_path,map_location=DEVICE))\n",
    "\n",
    "model_path = f'/workspace/{SAVE_ROOT}/{SAVE_FOLDAR}/best_model_title_level.pth'\n",
    "model_title_level.load_state_dict(torch.load(model_path,map_location=DEVICE))\n",
    "\n",
    "\n",
    "model_font = ResNet50(class_num=2094,feat_dim=300).to(DEVICE)\n",
    "font_model_path = \"/workspace/results/ResNet50_sw128_h64_lr0.001_v3/best_model.pth\"\n",
    "model_font.load_state_dict(torch.load(font_model_path,map_location=DEVICE))\n",
    "\n",
    "model_path = f'/workspace/{SAVE_ROOT}/{SAVE_FOLDAR}/best_model_coord.pth'\n",
    "embed_xy.load_state_dict(torch.load(model_path,map_location=DEVICE))\n",
    "\n",
    "model_path = f'/workspace/{SAVE_ROOT}/{SAVE_FOLDAR}/best_model_size.pth'\n",
    "embed_size.load_state_dict(torch.load(model_path,map_location=DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2795ef9f",
   "metadata": {},
   "source": [
    "# attention vis. (title level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca495fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAAsCAYAAAD4vrudAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABE0lEQVR4nO3YQQrCMBQAUSve/8Rq3CuCoCWN894y/MDPpgzdxhjjBABknWcvAADMJQYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIi7fHX7fn89u93Wm/vk7mr7rjp3lF1W23ePuSPtsvfO//qumXNH2uWX3/Mjv+vd3PX6evbEnwEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEiQEAiBMDABAnBgAgTgwAQJwYAIA4MQAAcWIAAOLEAADEbWOMMXsJAGAefwYAIE4MAECcGACAODEAAHFiAADixAAAxIkBAIgTAwAQJwYAIO4BbkvOUZr9ZdAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "color = [[(255,255-int(255*0.01*i),255-int(255*0.01*i)) for i in range(100)] for j in range(5)]\n",
    "color = np.array(color)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(color)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba3f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(f\"/workspace/dataset/AmazonBookCoverImages/test.csv\")\n",
    "df_test = df_test[df_test['split']=='test'].reset_index(drop=True)\n",
    "df_test = df_test[df_test['hight']>14]\n",
    "df_test = df_test[df_test['width']>14]\n",
    "df_test_org = pd.read_csv(f'/workspace/dataset/book30-listing-test.csv'\\\n",
    "                 ,encoding='cp932',names=(\"Amazon ID (ASIN)\",\"Filename\",\"Image URL\",\"Title\",\"Author\",\"Category ID\",\"Category\"))\n",
    "df_test = pd.merge(df_test_org,df_test,on=\"Filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d5a634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = df_test[\"Category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [11:22<00:00, 22.74s/it]\n"
     ]
    }
   ],
   "source": [
    "torch_fix_seed(seed=0)\n",
    "token_num = 16\n",
    "\n",
    "\n",
    "CONFIGS[\"title_level_config\"][\"transformer\"][\"num_layers\"] = NUM_LAYER\n",
    "title_level_config = CONFIGS[\"title_level_config\"]\n",
    "model_semantic_only = VisionTransformer(title_level_config, num_classes=32, zero_head=False, vis=True).to(DEVICE)\n",
    "model_path = '/workspace/results_for_multimodal_20230627/SemanticOnly_head6_layer4_lr1e-05_v1/best_model_semantic_only.pth'\n",
    "model_semantic_only.load_state_dict(torch.load(model_path,map_location=DEVICE))\n",
    "\n",
    "model_word_level.eval()\n",
    "model_title_level.eval()\n",
    "model_font.eval()\n",
    "model_semantic_only.eval()\n",
    "embed_size.eval() \n",
    "embed_xy.eval()   \n",
    "df_res =  df_test[['folder','Title','Category','Category ID']].copy()\n",
    "df_res = df_res[~df_res.duplicated()].reset_index(drop=True)\n",
    "\n",
    "for i in range(30):\n",
    "    #torch.argmax(y,1).item()\n",
    "    k = str(i+1)\n",
    "    col = f'top{k}_res'\n",
    "    df_res[col] = 100\n",
    "                \n",
    "for category in tqdm(category_list):\n",
    "    book_foldar_list = df_res[df_res['Category']==category]['folder'].values\n",
    "    label_list = []\n",
    "    all_vec_list = []\n",
    "    all_title_list = []\n",
    "    zero_vec = torch.zeros((len(ELEMENTS),300)).to(DEVICE)\n",
    "    attn_dict = {'foldar':[],'title':[],'elements':[],'word':[]}\n",
    "    with torch.no_grad():\n",
    "        for book_foldar in book_foldar_list:\n",
    "            word_vec_list = []\n",
    "            vec_list = []\n",
    "            word_list = ''\n",
    "            df_tmp = df_test[df_test['folder']==book_foldar].reset_index(drop=True)\n",
    "            label = df_tmp.loc[0,'Category ID']\n",
    "            category = df_tmp.loc[0,'Category']\n",
    "            filename = df_tmp.loc[0,'Filename']\n",
    "            foldar = book_foldar\n",
    "            for j in range(token_num): \n",
    "                if len(df_tmp)>j: \n",
    "                    word = df_tmp['word'].values[j]\n",
    "                    word_list += word\n",
    "                    w2v = model_semantic[word]\n",
    "                    semantic_vec = (torch.from_numpy(w2v.astype(np.float32)).clone()).to(DEVICE)\n",
    "                    w2v_torch = (torch.from_numpy(w2v.astype(np.float32)).clone()).to(DEVICE)\n",
    "                    word_vec_list.append(w2v_torch)\n",
    "                    # フォントの取得\n",
    "                    img_name = df_tmp['img_name'].values[j]\n",
    "                    img_path = '/workspace/dataset/word_detection_from_bookCover/dataset/'+category+'/word/' + foldar + '/'+ img_name\n",
    "                    if not os.path.isfile(img_path):\n",
    "                        img_path = '/workspace/dataset/CannotRead/word/' + foldar + '/'+ img_name\n",
    "                    img = get_img(img_path) \n",
    "                    _, font_vec = model_font(img.to(DEVICE))\n",
    "                    font_vec = font_vec.reshape(-1).detach()\n",
    "                    # 色の取得\n",
    "                    ch_color, bk_color = get_color(img_path)\n",
    "\n",
    "                    # 高さの取得\n",
    "                    height = torch.tensor([df_tmp.loc[j,\"hight\"]/df_tmp.loc[j,\"book_cover_hight\"]])\n",
    "                    x_height = embed_size(height.to(DEVICE,torch.float32))\n",
    "\n",
    "                    # 座標の取得\n",
    "                    coord = torch.tensor([df_tmp.loc[j,\"coord_x\"]/df_tmp.loc[j,\"book_cover_width\"],df_tmp.loc[j,\"coord_y\"]/df_tmp.loc[j,\"book_cover_hight\"]])\n",
    "                    x_coord = embed_xy(coord.to(DEVICE,torch.float32))\n",
    "                    \n",
    "                    data_list = []\n",
    "                    used_elements = []\n",
    "                    if \"semantic\" in ELEMENTS:\n",
    "                        data_list.append(semantic_vec)\n",
    "                        used_elements.append(\"semantic\")\n",
    "                    if \"font_style\" in ELEMENTS:\n",
    "                        data_list.append(font_vec)\n",
    "                        used_elements.append(\"font_style\")\n",
    "                    if \"char_color\" in ELEMENTS:\n",
    "                        data_list.append(ch_color.to(DEVICE))\n",
    "                        used_elements.append(\"char_color\")\n",
    "                    if \"bk_color\" in ELEMENTS:\n",
    "                        data_list.append(bk_color.to(DEVICE))\n",
    "                        used_elements.append(\"bk_color\")\n",
    "                    if \"height\" in ELEMENTS:\n",
    "                        data_list.append(x_height)\n",
    "                        used_elements.append(\"height\")\n",
    "                    if \"coord\" in ELEMENTS:\n",
    "                        data_list.append(x_coord)\n",
    "                        used_elements.append(\"coord\")\n",
    "                    feat_vec = torch.stack(data_list)\n",
    "                    vec_list.append(feat_vec)\n",
    "                    word_list += ' '\n",
    "                    assert used_elements == ELEMENTS, \"not match between used feat. and selected feat.\"\n",
    "                    \n",
    "                else: #ない場合はゼロベクトルを入れる\n",
    "                    word_vec_list.append(torch.zeros((300)).to(DEVICE))\n",
    "                    vec_list.append(zero_vec)\n",
    "                    word_list += '* '\n",
    "            vec_list = torch.stack(vec_list)\n",
    "            word_vec_list = torch.stack(word_vec_list)\n",
    "            x_word,_ = model_word_level(vec_list) \n",
    "            y,_ = model_title_level(x_word.reshape(1,16,300))\n",
    "            for i in range(30):\n",
    "                topk_res = torch.argsort(y,descending=True)[0,i].item()\n",
    "                #torch.argmax(y,1).item()\n",
    "                k = str(i+1)\n",
    "                col = f'top{k}_res'\n",
    "                df_res.loc[df_res['folder']==foldar,col] = int(topk_res)\n",
    "            attn,logits = visualize(x_word.reshape(1,16,300),model_title_level.to(DEVICE),device=DEVICE)\n",
    "            attn_element,logits_element = visualize(vec_list.reshape(16,len(ELEMENTS),300),model_word_level.to(DEVICE),device=DEVICE)\n",
    "            mask_semantic,_ = visualize(word_vec_list.reshape(1,16,300),model_semantic_only.to(DEVICE),device=DEVICE)\n",
    "\n",
    "            img_all = 0\n",
    "            idx = 0\n",
    "            words = word_list.split(' ')\n",
    "            for i in range(len(words)):\n",
    "                word = words[i]\n",
    "                if word == '':\n",
    "                    break\n",
    "                else:\n",
    "                    rate = attn[:,i].item()\n",
    "                    rate_design = attn_element[i,:].cpu().tolist()\n",
    "                    rate_semantic = mask_semantic[:,i].item()\n",
    "                    img = get_visualization_res(word,rate,rate_design,width=180,height=120,element_num=len(ELEMENTS))\n",
    "                    w,h,_ = img.shape\n",
    "                    img_semantic = get_word_img(word,rate_semantic,width=h,height=w,ttf=ttf_path)\n",
    "                    if i == 0:  \n",
    "                        img_all = img\n",
    "                        img_semantic_all = img_semantic\n",
    "                    else:\n",
    "                        img_all = np.hstack((img_all,img))\n",
    "                        img_semantic_all = np.hstack((img_semantic_all,img_semantic))\n",
    "\n",
    "            img_all = np.vstack((img_semantic_all,img_all))\n",
    "            book_id = foldar.split(\"res_\")[-1]\n",
    "            book_path =  f'/workspace/dataset/AmazonBookCoverImages/genres/{category}/{book_id}.jpg'\n",
    "            detect_book_path =  f'/workspace/dataset/word_detection_from_bookCover/dataset/{category}/ditection/{book_foldar}.jpg'\n",
    "            if not os.path.isfile(book_path):\n",
    "                book_path = f'/workspace/dataset/AmazonBookCoverImages/CannotRead/{book_id}.jpg'\n",
    "                detect_book_path = f'/workspace/dataset/CannotRead/ditection/{book_foldar}.jpg'\n",
    "            os.makedirs(f'/workspace/{SAVE_ROOT}/bookcover/{category}/',exist_ok=True)\n",
    "            save_book_path = f'/workspace/{SAVE_ROOT}/bookcover/{category}/{book_foldar}.png'\n",
    "            shutil.copy(book_path, save_book_path)\n",
    "            os.makedirs(f'/workspace/{SAVE_ROOT}/detect_bookcover/{category}/',exist_ok=True)\n",
    "            save_detect_book_path = f'/workspace/{SAVE_ROOT}/detect_bookcover/{category}/{book_foldar}.png'\n",
    "            shutil.copy(detect_book_path, save_detect_book_path)\n",
    "\n",
    "            attn_dict['foldar'].append(foldar)\n",
    "            attn_dict['elements'].append(attn_element)\n",
    "            attn_dict['word'].append(attn)\n",
    "            attn_dict['title'].append(words)\n",
    "            os.makedirs(f'/workspace/{SAVE_ROOT}/visualization_res/{SAVE_FOLDAR}/{category}/',exist_ok=True)\n",
    "            img_all = cv2.cvtColor(img_all, cv2.COLOR_BGR2RGB)\n",
    "            cv2.imwrite(f'/workspace/{SAVE_ROOT}/visualization_res/{SAVE_FOLDAR}/{category}/{book_foldar}.png', img_all)\n",
    "        \n",
    "        os.makedirs(f'/workspace/{SAVE_ROOT}/attention/{SAVE_FOLDAR}/',exist_ok=True)\n",
    "        with open(f'/workspace/{SAVE_ROOT}/attention/{SAVE_FOLDAR}/{category}.pickle',\"wb\") as f:\n",
    "            pickle.dump(attn_dict,f)\n",
    "shutil.make_archive(f'/workspace/{SAVE_ROOT}/visualization_res', 'zip', root_dir=f'/workspace/{SAVE_ROOT}/visualization_res')\n",
    "shutil.make_archive(f'/workspace/{SAVE_ROOT}/bookcover', 'zip', root_dir=f'/workspace/{SAVE_ROOT}/bookcover')\n",
    "name = \"\"\n",
    "for i in range(len(DROP_ELEMENTS)):\n",
    "    name += DROP_ELEMENTS[i]\n",
    "os.makedirs(f'/workspace/{SAVE_ROOT}/csv/',exist_ok=True)\n",
    "# df_res.to_csv(f'/workspace/{SAVE_ROOT}/csv/{name}_res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c9d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db6042",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "for k in [3,5]:\n",
    "    correct_num = 0\n",
    "    for i in range(len(df_res)):\n",
    "        pred_list = [df_res.loc[i,f\"top{j}_res\"] for j in range(1,k+1)]\n",
    "        true_label = df_res.loc[i,\"Category ID\"]\n",
    "        if true_label in pred_list:\n",
    "            correct_num += 1\n",
    "    print(correct_num/len(df_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a16ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_res.to_csv('/workspace/{SAVE_ROOT}/all_res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    category = df_res[df_res[\"Category ID\"]==i]['Category'].values[0]\n",
    "    total_num = len(df_res[df_res[\"Category ID\"]==i]['Category'])\n",
    "    true_num = len(df_res[(df_res[\"Category ID\"]==i) & (df_res[\"Category ID\"]==df_res[\"top1_res\"])])\n",
    "    acc = round(true_num/total_num*100,4)\n",
    "    print(category, f'{true_num}/{total_num}:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_res[[\"Category ID\",\"Category\"]]\n",
    "df_tmp = df_tmp[~df_tmp.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917041cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_res[\"Category ID\"].values\n",
    "y_pred = df_res[\"top1_res\"].values\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "xtics = df_tmp.sort_values(\"Category ID\")['Category'].values\n",
    "plt.figure(figsize = (14,10))\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "sns.heatmap(cm, cmap='Blues',xticklabels=xtics,yticklabels=xtics)#,annot=True\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524bea05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
